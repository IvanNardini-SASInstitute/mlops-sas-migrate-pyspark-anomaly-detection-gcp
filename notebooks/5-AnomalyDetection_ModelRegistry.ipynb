{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Anomaly detection in cellular networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The purpose of this notebook is to solve a anomaly detection problem proposed as a competition in the Kaggle InClass platform.\n",
    "\n",
    "## Problem description\n",
    "\n",
    "### Context:\n",
    "\n",
    "Traditionally, the design of a cellular network focuses on the optimization of energy and resources that guarantees a smooth operation even during peak hours (i.e. periods with higher traffic load). \n",
    "However, this implies that cells are most of the time overprovisioned of radio resources. \n",
    "Next generation cellular networks ask for a dynamic management and configuration in order to adapt to the varying user demands in the most efficient way with regards to energy savings and utilization of frequency resources. \n",
    "If the network operator were capable of anticipating to those variations in the users’ traffic demands, a more efficient management of the scarce (and expensive) network resources would be possible.\n",
    "Current research in mobile networks looks upon Machine Learning (ML) techniques to help manage those resources. \n",
    "In this case, you will explore the possibilities of ML to detect abnormal behaviors in the utilization of the network that would motivate a change in the configuration of the base station.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "The objective of the network optimization team is to analyze traces of past activity, which will be used to train an ML system capable of classifying samples of current activity as:\n",
    " - 0 (normal): current activity corresponds to normal behavior of any working day and. Therefore, no re-configuration or redistribution of resources is needed.\n",
    " - 1 (unusual): current activity slightly differs from the behavior usually observed for that time of the day (e.g. due to a strike, demonstration, sports event, etc.), which should trigger a reconfiguration of the base station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset has been obtained from a real LTE deployment. During two weeks, different metrics were gathered from a set of 10 base stations, each having a different number of cells, every 15 minutes. \n",
    "\n",
    "The dataset is provided in the form of a csv file, where each row corresponds to a sample obtained from one particular cell at a certain time. Each data example contains the following features:\n",
    "\n",
    " - Time : hour of the day (in the format hh:mm) when the sample was generated.\n",
    " - CellName1: text string used to uniquely identify the cell that generated the current sample. CellName is in the form xαLTE, where x identifies the base station, and α the cell within that base station (see the example in the right figure).\n",
    " - PRBUsageUL and PRBUsageDL: level of resource utilization in that cell measured as the portion of Physical Radio Blocks (PRB) that were in use (%) in the previous 15 minutes. Uplink (UL) and downlink (DL) are measured separately.\n",
    " - meanThrDL and meanThrUL: average carried traffic (in Mbps) during the past 15 minutes. Uplink (UL) and downlink (DL) are measured separately.\n",
    " - maxThrDL and maxThrUL: maximum carried traffic (in Mbps) measured in the last 15 minutes. Uplink (UL) and downlink (DL) are measured separately.\n",
    " - meanUEDL and meanUEUL: average number of user equipment (UE) devices that were simultaneously active during the last 15 minutes. Uplink (UL) and downlink (DL) are measured separately.\n",
    " - maxUEDL and maxUEUL: maximum number of user equipment (UE) devices that were simultaneously active during the last 15 minutes. Uplink (UL) and downlink (DL) are measured separately.\n",
    " - maxUE_UL+DL: maximum number of user equipment (UE) devices that were active simultaneously in the last 15 minutes, regardless of UL and DL.\n",
    " - Unusual: labels for supervised learning. A value of 0 determines that the sample corresponds to normal operation, a value of 1 identifies unusual behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from IPython.display import Image\n",
    "\n",
    "#Analysis\n",
    "import pyspark\n",
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    print('WARN: Something wrong with pyspark library. Please check configuration settings!')\n",
    "from pyspark.sql.types import DoubleType\n",
    "    \n",
    "#Feature Engineering\n",
    "from pyspark.sql.functions import col, when, lit, array, explode, rand, udf\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "#Model Training\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "#Model Registry\n",
    "import pandas as pd\n",
    "from sasctl import pzmm\n",
    "from sasctl import Session\n",
    "from sasctl.services import model_repository\n",
    "import getpass\n",
    "\n",
    "    \n",
    "# Reloads functions each time so you can edit a script and not need to restart the kernel\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-22T23:24:55.390680Z",
     "start_time": "2019-02-22T23:24:55.344380Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_root_dir (src: str, max_nest: int) -> str:\n",
    "    '''\n",
    "    Specify paths and appending directories\n",
    "    with relevant python source code.\n",
    "    :param src: the path of the source\n",
    "    :param max_nest: number of levels to search for the src\n",
    "    :return: root_dir path of the root\n",
    "    '''\n",
    "    root_dir = os.curdir\n",
    "    nest = 0\n",
    "    while src not in os.listdir(root_dir) and nest < max_nest:\n",
    "        root_dir = os.path.join(os.pardir, root_dir)  # Look up the directory structure for a src directory\n",
    "        nest += 1\n",
    "    # If you don't find the src directory, the root directory is this directory\n",
    "    root_dir = os.path.abspath(root_dir) if nest < max_nest else os.path.abspath(\n",
    "        os.curdir)\n",
    "    return root_dir\n",
    "\n",
    "def set_src (root_dir: str, src: str) -> str:\n",
    "    '''\n",
    "     Get the source directory and append\n",
    "     path to access python packages/scripts within directory\n",
    "    :param root_dir: root path\n",
    "    :param src: src path\n",
    "    :return: last system path record (to check)\n",
    "    '''\n",
    "    if src in os.listdir(root_dir):\n",
    "        src_dir = os.path.join(root_dir, src)\n",
    "        sys.path.append(src_dir)\n",
    "    return sys.path[-1]\n",
    "\n",
    "\n",
    "def set_folder (root_dir: str, folder: str) -> str:\n",
    "    '''\n",
    "    Set the folder path based on the folder name\n",
    "    :param root_dir: root path\n",
    "    :param folder: folder name\n",
    "    :return: folder_path from root\n",
    "    '''\n",
    "    folder_path = os.path.join(\n",
    "        root_dir, folder) if folder in os.listdir(root_dir) else os.curdir\n",
    "    return folder_path\n",
    "\n",
    "def set_path(path:str, dirname:str) -> str:\n",
    "    '''\n",
    "    Set the entire path given a directory name\n",
    "    :param path: \n",
    "    :param dirname: \n",
    "    :return: new path\n",
    "    '''\n",
    "    return os.path.join(path, dirname)\n",
    "\n",
    "\n",
    "def unzip (inpath: str, outpath: str) -> None:\n",
    "    '''\n",
    "    unzip a compressed file\n",
    "    :param inpath: path of zip\n",
    "    :param outpath: path to unzip\n",
    "    :return: None\n",
    "    '''\n",
    "    zf = ZipFile(inpath, 'r')\n",
    "    zf.extractall(outpath)\n",
    "    zf.close()\n",
    "    \n",
    "def metrics (dataframe, actual, predicted):\n",
    "    '''\n",
    "    Calculates evaluation metrics from predicted results\n",
    "    :param dataframe: spark.sql.dataframe with the real and predicted values\n",
    "    :param actual: Name of column with observed target values\n",
    "    :param predicted: Name of column with predicted values\n",
    "    :return: None\n",
    "    '''\n",
    "\n",
    "    # Along each row are the actual values and down each column are the predicted\n",
    "    dataframe = dataframe.withColumn(actual, col(actual).cast('integer'))\n",
    "    dataframe = dataframe.withColumn(predicted, col(predicted).cast('integer'))\n",
    "    cm = dataframe.crosstab(actual, predicted)\n",
    "    cm = cm.sort(cm.columns[0], ascending=True)\n",
    "\n",
    "    # Adds missing column in case just one class was predicted\n",
    "    if not '0' in cm.columns:\n",
    "        cm = cm.withColumn('0', lit(0))\n",
    "    if not '1' in cm.columns:\n",
    "        cm = cm.withColumn('1', lit(0))\n",
    "\n",
    "    # Subsets values from confusion matrix\n",
    "    zero = cm.filter(cm[cm.columns[0]] == 0.0)\n",
    "    first_0 = zero.take(1)\n",
    "\n",
    "    one = cm.filter(cm[cm.columns[0]] == 1.0)\n",
    "    first_1 = one.take(1)\n",
    "\n",
    "    tn = first_0[0][1]\n",
    "    fp = first_0[0][2]\n",
    "    fn = first_1[0][1]\n",
    "    tp = first_1[0][2]\n",
    "\n",
    "    # Calculate metrics from values in the confussion matrix\n",
    "    if (tp == 0):\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = 0\n",
    "        spe = float((tn) / (tn + fp))\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "        f1 = 0\n",
    "    elif (tn == 0):\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = float((tp) / (tp + fn))\n",
    "        spe = 0\n",
    "        prec = float((tp) / (tp + fp))\n",
    "        rec = float((tp) / (tp + fn))\n",
    "        f1 = 2 * float((prec * rec) / (prec + rec))\n",
    "    else:\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = float((tp) / (tp + fn))\n",
    "        spe = float((tn) / (tn + fp))\n",
    "        prec = float((tp) / (tp + fp))\n",
    "        rec = float((tp) / (tp + fn))\n",
    "        f1 = 2 * float((prec * rec) / (prec + rec))\n",
    "\n",
    "    # Print results\n",
    "    print('Confusion Matrix and Statistics: \\n')\n",
    "    cm.show()\n",
    "\n",
    "    print('True Positives:', tp)\n",
    "    print('True Negatives:', tn)\n",
    "    print('False Positives:', fp)\n",
    "    print('False Negatives:', fn)\n",
    "    print('Total:', dataframe.count(), '\\n')\n",
    "\n",
    "    print('Accuracy: {0:.2f}'.format(acc))\n",
    "    print('Sensitivity: {0:.2f}'.format(sen))\n",
    "    print('Specificity: {0:.2f}'.format(spe))\n",
    "    print('Precision: {0:.2f}'.format(prec))\n",
    "    print('Recall: {0:.2f}'.format(rec))\n",
    "    print('F1-score: {0:.2f}'.format(f1))\n",
    "    # Create spark dataframe with results\n",
    "    l = [(acc, sen, spe, prec, rec, f1)]\n",
    "    df = spark.createDataFrame(l, ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'Recall', 'F1'])\n",
    "\n",
    "    return (df)\n",
    "\n",
    "def get_output_variables(names, labels, eventprob):\n",
    "    '''\n",
    "    Given variable names, labels and event probability, \n",
    "    it creates dataframes for pzmm metadata generation\n",
    "    :param names: \n",
    "    :param labels: \n",
    "    :param eventprob: \n",
    "    :return: outputVar\n",
    "    '''\n",
    "    outputVar = pd.DataFrame(columns=names)\n",
    "    outputVar[names[0]] = [random.random(), random.random()]\n",
    "    outputVar[names[1]] = [random.random(), random.random()]\n",
    "    outputVar[names[2]] = labels\n",
    "    outputVar[names[3]] = eventprob\n",
    "    return outputVar\n",
    "\n",
    "def zip_folder(folder_to_zip_path, rmtree=False):\n",
    "    '''\n",
    "    Given the folder to zip path,\n",
    "    create an archive\n",
    "    :param folder_to_zip_path: \n",
    "    :param rmtree: \n",
    "    :return: zipath\n",
    "    '''\n",
    "    path_sep = '/'\n",
    "    root_dir = path_sep.join(folder_to_zip_path.split('/')[:-1])\n",
    "    base_dir = folder_to_zip_path.split('/')[-1]\n",
    "    zipath = shutil.make_archive(\n",
    "        folder_to_zip_path,         # folder to zip\n",
    "        'zip',                      # the archive format - or tar, bztar, gztar \n",
    "        root_dir=root_dir,          # folder to zip root\n",
    "        base_dir=base_dir)          # folder to zip name\n",
    "    if rmtree:\n",
    "        shutil.rmtree(folder_to_zip_path) # remove .zip folder\n",
    "    return zipath\n",
    "\n",
    "extract1_udf = udf(lambda value: value[1].item(), DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = get_root_dir('src', 5)\n",
    "src_dir = set_src(root_dir, 'src')\n",
    "data_dir = set_folder(root_dir, 'data')\n",
    "raw_data_dir = set_path(data_dir, 'raw')\n",
    "interim_data_dir = set_path(data_dir, 'interim')\n",
    "processed_data_dir = set_path(data_dir, 'processed')\n",
    "figures_dir = set_folder(root_dir, 'figures')\n",
    "features_dir = set_folder(root_dir, 'features')\n",
    "index_features_dir = set_path(features_dir, 'indexstr')\n",
    "ohe_features_dir = set_path(features_dir, 'ohe')\n",
    "std_features_dir = set_path(features_dir, 'std')\n",
    "models_dir = set_folder(root_dir, 'models')\n",
    "deliverables_path = set_folder(root_dir, 'deliverables')\n",
    "model_version_dir = set_path(deliverables_path, 'pyspark_GBTClassifier')\n",
    "gbt_pipe_dir = set_path(model_version_dir, 'gbt_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not exists create a spark session named Anomaly Detection where the master node is local\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Anomaly Detection\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.getActiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = set_path(processed_data_dir, 'ML-MATT-CompetitionQT1920_train_processed.parquet')\n",
    "test_path = set_path(processed_data_dir, 'ML-MATT-CompetitionQT1920_test_processed.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = spark.read.parquet(train_path)\n",
    "test_df = spark.read.parquet(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_pipe = Pipeline.load(gbt_pipe_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test the pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model = gbt_pipe.fit(train_df)\n",
    "predictions_train = gbt_model.transform(train_df)\n",
    "predictions_train.select('CellName', 'features', 'Unusual', 'rawPrediction', 'probability', 'prediction').show(5)\n",
    "metrics(predictions_train, 'Unusual', 'prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = gbt_model.transform(test_df)\n",
    "predictions_test.select('CellName', 'features', 'Unusual', 'rawPrediction', 'probability', 'prediction').show(5)\n",
    "metrics(predictions_test, 'Unusual', 'prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Model for versioning in SAS Model Manager\n",
    "\n",
    "As always, we create some metadata files to take advantages from SAS Model Manager.\n",
    "In this case, because we want to migrate workload on GCP, we need as much info as possible. So we create:\n",
    "\n",
    "1. requirement.txt\n",
    "2. *.py with etl, train model and score data\n",
    "3. *.json with all metainfo (inputs, outputs, model properties...)\n",
    "4. others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../deliverables/pyspark_GBTClassifier/requirement.txt\n",
    "numpy==1.19.4\n",
    "pandas==1.1.4\n",
    "pyspark==3.0.1\n",
    "PyYAML==5.3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *.json with all metainfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base metainfo\n",
    "\n",
    "inSample = train_df.toPandas()[:1]\n",
    "js = pzmm.JSONFiles()\n",
    "\n",
    "# inputVar.json\n",
    "js.writeVarJSON(inSample, isInput=True, jPath=model_version_dir)\n",
    "\n",
    "# outputVar.json\n",
    "names=['P_UNUSUAL0', 'P_UNUSUAL1', 'EM_CLASSIFICATION', 'EM_EVENTPROBABILITY']\n",
    "labels=['0', '1']\n",
    "eventprob=0.5\n",
    "outSample = get_output_variables(names, labels, eventprob)\n",
    "js.writeVarJSON(outSample, isInput=False, jPath=model_version_dir)\n",
    "\n",
    "# ModelProperties.json\n",
    "modelname='pyspark_gbtClassifier'\n",
    "target='Unusual'\n",
    "predictors=['CellName', 'hour', 'minutes', 'PRBUsageUL', 'PRBUsageDL', \n",
    "            'meanThr_DL', 'meanThr_UL', 'maxThr_DL', 'maxThr_UL', 'meanUE_DL', \n",
    "            'meanUE_UL', 'maxUE_DL', 'maxUE_UL']\n",
    "js.writeModelPropertiesJSON(modelName=modelname,\n",
    "                                   modelDesc='A pyspark GBTClassifier for Network anomaly detection',\n",
    "                                   targetVariable=target,\n",
    "                                   modelType='Boosted Tree',\n",
    "                                   modelPredictors=predictors,\n",
    "                                   targetEvent=1,\n",
    "                                   numTargetCategories=1,\n",
    "                                   eventProbVar='EM_EVENTPROBABILITY',\n",
    "                                   jPath=model_version_dir,\n",
    "                                   modeler='ivnard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced metainfo\n",
    "\n",
    "dmcas_fitstat.json\n",
    "trainData = predictions_train.withColumn('p2', extract1_udf('probability')) \\\n",
    "                             .select('Unusual', 'p2').toPandas()\n",
    "testData = predictions_test.withColumn('p2', extract1_udf('probability')) \\\n",
    "                             .select('Unusual', 'p2').toPandas()\n",
    "js.calculateFitStat(trainData=trainData, testData=testData, jPath=model_version_dir)\n",
    "\n",
    "dmcas_roc.json, dmcas_lift.json\n",
    "print('Provide username and password to Viya Server login')\n",
    "username = getpass.getpass()\n",
    "password = getpass.getpass()\n",
    "host = 'rusid1.rus.sas.com'\n",
    "sess = Session(host, username, password, verify_ssl=False, protocol='http')\n",
    "conn = sess.as_swat()\n",
    "js.generateROCLiftStat(target, 1, conn, trainData=trainData, testData=testData, jPath=model_version_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *.py for train model and score data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../deliverables/pyspark_GBTClassifier/train.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "train.py is the training module for our project.\n",
    "Remarks: The model package is designed in a way\n",
    "it's executable both in SAS Viya and GCP Dataproc\n",
    "\n",
    "Steps:\n",
    "1 - Read data (both server and gcp cloud storage)\n",
    "2 - Build Machine Learning Pipeline\n",
    "3 - Serialize the Pipeline and all data\n",
    "\n",
    "Author: Ivan Nardini (ivan.nardini@sas.com)\n",
    "\"\"\"\n",
    "\n",
    "# Libraries ------------------------------------------------------------------------------------------------------------\n",
    "import logging\n",
    "import logging.config\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "#from helpers import read_parquet, write_parquet, metrics, save_pipeline\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    print('WARN: Something wrong with pyspark library. Please check configuration settings!')\n",
    "\n",
    "\n",
    "# Helpers --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def read_parquet (session: SparkSession, filepath: str) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Read a parquet file\n",
    "    :param session: SparkSession\n",
    "    :param filepath: the path of parquet datafile\n",
    "    :return: pyspark.sql.DataFrame\n",
    "    '''\n",
    "    return session.read.parquet(filepath)\n",
    "\n",
    "\n",
    "def write_parquet (df: pyspark.sql.DataFrame, filepath: str) -> None:\n",
    "    '''\n",
    "    Write a parquet file\n",
    "    :param df: DataFrame to store\n",
    "    :param filepath: the path of parquet datafile\n",
    "    :return: None\n",
    "    '''\n",
    "    df.write.mode('overwrite').save(filepath)\n",
    "\n",
    "\n",
    "def metrics (session: SparkSession, dataframe: pyspark.sql.DataFrame, actual: str,\n",
    "             predicted: str) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Calculates evaluation metrics from predicted results\n",
    "\n",
    "    :param dataframe: spark.sql.dataframe with the real and predicted values\n",
    "    :param actual: Name of column with observed target values\n",
    "    :param predicted: Name of column with predicted values\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # Along each row are the actual values and down each column are the predicted\n",
    "    dataframe = dataframe.withColumn(actual, col(actual).cast('integer'))\n",
    "    dataframe = dataframe.withColumn(predicted, col(predicted).cast('integer'))\n",
    "    cm = dataframe.crosstab(actual, predicted)\n",
    "    cm = cm.sort(cm.columns[0], ascending=True)\n",
    "\n",
    "    # Adds missing column in case just one class was predicted\n",
    "    if not '0' in cm.columns:\n",
    "        cm = cm.withColumn('0', lit(0))\n",
    "    if not '1' in cm.columns:\n",
    "        cm = cm.withColumn('1', lit(0))\n",
    "\n",
    "    # Subsets values from confusion matrix\n",
    "    zero = cm.filter(cm[cm.columns[0]] == 0.0)\n",
    "    first_0 = zero.take(1)\n",
    "\n",
    "    one = cm.filter(cm[cm.columns[0]] == 1.0)\n",
    "    first_1 = one.take(1)\n",
    "\n",
    "    tn = first_0[0][1]\n",
    "    fp = first_0[0][2]\n",
    "    fn = first_1[0][1]\n",
    "    tp = first_1[0][2]\n",
    "\n",
    "    # Calculate metrics from values in the confussion matrix\n",
    "    if (tp == 0):\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = 0\n",
    "        spe = float((tn) / (tn + fp))\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "        f1 = 0\n",
    "    elif (tn == 0):\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = float((tp) / (tp + fn))\n",
    "        spe = 0\n",
    "        prec = float((tp) / (tp + fp))\n",
    "        rec = float((tp) / (tp + fn))\n",
    "        f1 = 2 * float((prec * rec) / (prec + rec))\n",
    "    else:\n",
    "        acc = float((tp + tn) / (tp + tn + fp + fn))\n",
    "        sen = float((tp) / (tp + fn))\n",
    "        spe = float((tn) / (tn + fp))\n",
    "        prec = float((tp) / (tp + fp))\n",
    "        rec = float((tp) / (tp + fn))\n",
    "        f1 = 2 * float((prec * rec) / (prec + rec))\n",
    "\n",
    "    # Print results\n",
    "    print('Confusion Matrix and Statistics: \\n')\n",
    "    cm.show()\n",
    "\n",
    "    print('True Positives:', tp)\n",
    "    print('True Negatives:', tn)\n",
    "    print('False Positives:', fp)\n",
    "    print('False Negatives:', fn)\n",
    "    print('Total:', dataframe.count(), '\\n')\n",
    "\n",
    "    print('Accuracy: {0:.2f}'.format(acc))\n",
    "    print('Sensitivity: {0:.2f}'.format(sen))\n",
    "    print('Specificity: {0:.2f}'.format(spe))\n",
    "    print('Precision: {0:.2f}'.format(prec))\n",
    "    print('Recall: {0:.2f}'.format(rec))\n",
    "    print('F1-score: {0:.2f}'.format(f1))\n",
    "\n",
    "    # Create spark dataframe with results\n",
    "    l = [(acc, sen, spe, prec, rec, f1)]\n",
    "    df = session.createDataFrame(l, ['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'Recall', 'F1'])\n",
    "    return df\n",
    "\n",
    "\n",
    "extract0_udf = udf(lambda value: value[0].item(), DoubleType())\n",
    "extract1_udf = udf(lambda value: value[1].item(), DoubleType())\n",
    "\n",
    "def save_pipeline(pipeline: PipelineModel, filepath:str) -> None:\n",
    "    '''\n",
    "    Serialize the fitted pipeline\n",
    "    :param pipeline:\n",
    "    :param filepath:\n",
    "    :return: None\n",
    "    '''\n",
    "    pipeline.write().overwrite().save(path=filepath)\n",
    "\n",
    "# Builders -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def build_pipeline (pipeconfig: dict) -> pyspark.ml.Pipeline:\n",
    "    '''\n",
    "    Build a Pipeline instance based on config file\n",
    "    :param pipeconfig: metadata dictionary\n",
    "    :return: pyspark.ml.Pipeline\n",
    "    '''\n",
    "\n",
    "    # Pipeline metadata\n",
    "    cats = pipeconfig['variables']['categoricals']\n",
    "    nums = pipeconfig['variables']['numericals']\n",
    "    index_names = pipeconfig['metadata']['index_names']\n",
    "    encoded_names = pipeconfig['metadata']['encoded_names']\n",
    "    vect_name = pipeconfig['metadata']['vect_name']\n",
    "    feats_name = pipeconfig['metadata']['feats_name']\n",
    "    labelcol = pipeconfig['model']['labelCol']\n",
    "    maxdepth = pipeconfig['model']['maxDepth']\n",
    "    maxbins = pipeconfig['model']['maxBins']\n",
    "    maxiter = pipeconfig['model']['maxIter']\n",
    "    seed = pipeconfig['model']['seed']\n",
    "\n",
    "    # Build stages\n",
    "    stageone = StringIndexer(inputCols=cats,\n",
    "                             outputCols=index_names)\n",
    "\n",
    "    stagetwo = OneHotEncoder(dropLast=False,\n",
    "                             inputCols=stageone.getOutputCols(),\n",
    "                             outputCols=encoded_names)\n",
    "\n",
    "    stagethree = VectorAssembler(inputCols=nums + stagetwo.getOutputCols(),\n",
    "                                 outputCol=vect_name)\n",
    "\n",
    "    stagefour = MinMaxScaler(inputCol=stagethree.getOutputCol(),\n",
    "                             outputCol=feats_name)\n",
    "\n",
    "    stagefive = GBTClassifier(featuresCol=stagefour.getOutputCol(),\n",
    "                              labelCol=labelcol,\n",
    "                              maxDepth=maxdepth,\n",
    "                              maxBins=maxbins,\n",
    "                              maxIter=maxiter,\n",
    "                              seed=seed)\n",
    "    pipeline = Pipeline(stages=[stageone, stagetwo, stagethree, stagefour, stagefive])\n",
    "\n",
    "    return pipeline\n",
    "\n",
    "# Main -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def run_training (args):\n",
    "\n",
    "    # Read configuration\n",
    "    logging.info('Read config file.')\n",
    "    with open(args.configfile, \"r\") as cf:\n",
    "        config = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "    sparksession = config['sparksession']\n",
    "    data = config['data']\n",
    "    pipeline = config['pipeline']\n",
    "    output = config['output']\n",
    "\n",
    "    # Create a spark session\n",
    "    logging.info('Instantiate the {0} Spark session'.format(sparksession['appName']))\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(sparksession['master']) \\\n",
    "        .appName(sparksession['appName']) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load Data\n",
    "    logging.info('Load train and test data')\n",
    "    train_df = read_parquet(spark, data['train_datapath'])\n",
    "    test_df = read_parquet(spark, data['test_datapath'])\n",
    "\n",
    "    # Execute training\n",
    "    logging.info('Train {0} Pipeline'.format(pipeline['model']['method']))\n",
    "    train_pipe = build_pipeline(pipeline)\n",
    "    gbt_model = train_pipe.fit(train_df)\n",
    "\n",
    "    # Evaluate\n",
    "    logging.info('Evaluate the model')\n",
    "    predictions_test = gbt_model.transform(test_df)\n",
    "    predictions_test.select(output['showschema_train']).show(5)\n",
    "    metrics_df = metrics(spark, predictions_test, 'Unusual', 'prediction')\n",
    "\n",
    "    # Save training data\n",
    "    logging.info('Save all the process outputs')\n",
    "    # Save trained pipeline\n",
    "    logging.info('Saving pipeline...')\n",
    "    save_pipeline(gbt_model, output['pipeline_path'])\n",
    "    # Save predictions\n",
    "    logging.info('Saving predictions...')\n",
    "    predictions_test_fmt = predictions_test.withColumn('P_Unusual0', extract0_udf('probability')).withColumn(\n",
    "         'P_Unusual1', extract1_udf('probability')).select(output['columnschema_train'])\n",
    "    write_parquet(predictions_test_fmt, output['test_scored_path'])\n",
    "    # Save metrics\n",
    "    logging.info('Saving metrics...')\n",
    "    write_parquet(metrics_df, output['metrics_scored_path'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #logging.config.fileConfig(\"../../config/logging/local.conf\")\n",
    "    logging.basicConfig(format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                        datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    parser = argparse.ArgumentParser(description=\"Train Pyspark GBTClassifier\")\n",
    "    parser.add_argument('--configfile', required=True, help='path to configuration yaml file')\n",
    "    args = parser.parse_args()\n",
    "    run_training(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../deliverables/pyspark_GBTClassifier/score.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "score.py is the scoring module for our project.\n",
    "Remarks: The model package is designed in a way\n",
    "it's executable both in SAS Viya and GCP Dataproc\n",
    "\n",
    "Steps:\n",
    "1 - Read data (both server and gcp cloud storage)\n",
    "2 - Read serialized pipeline (both server and gcp cloud storage)\n",
    "3 - Score (or trasform) data\n",
    "4 - Store scored data (both server and gcp cloud storage)\n",
    "\n",
    "Author: Ivan Nardini (ivan.nardini@sas.com)\n",
    "\"\"\"\n",
    "\n",
    "# Libraries ------------------------------------------------------------------------------------------------------------\n",
    "import logging\n",
    "import logging.config\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "# from helpers import read_parquet, write_parquet, load_model\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    print('WARN: Something wrong with pyspark library. Please check configuration settings!')\n",
    "\n",
    "# Helpers --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def read_parquet (session: SparkSession, filepath: str) -> pyspark.sql.DataFrame:\n",
    "    '''\n",
    "    Read a parquet file\n",
    "    :param session: SparkSession\n",
    "    :param filepath: the path of parquet datafile\n",
    "    :return: pyspark.sql.DataFrame\n",
    "    '''\n",
    "    return session.read.parquet(filepath)\n",
    "\n",
    "\n",
    "def write_parquet (df: pyspark.sql.DataFrame, filepath: str) -> None:\n",
    "    '''\n",
    "    Write a parquet file\n",
    "    :param df: DataFrame to store\n",
    "    :param filepath: the path of parquet datafile\n",
    "    :return: None\n",
    "    '''\n",
    "    df.write.mode('overwrite').save(filepath)\n",
    "\n",
    "\n",
    "def load_model(filepath:str) -> PipelineModel:\n",
    "    '''\n",
    "    Load the fitted pipeline\n",
    "    :param filepath:\n",
    "    :return: PipelineModel\n",
    "    '''\n",
    "    return PipelineModel.load(filepath)\n",
    "\n",
    "extract0_udf = udf(lambda value: value[0].item(), DoubleType())\n",
    "extract1_udf = udf(lambda value: value[1].item(), DoubleType())\n",
    "\n",
    "# Builders -------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def score_model(data:pyspark.sql.DataFrame, model:PipelineModel) -> pyspark.sql.DataFrame:\n",
    "    predictions_test = model.transform(data)\n",
    "    return predictions_test\n",
    "\n",
    "# Main -----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def run_scoring(args):\n",
    "    # Read Configuration\n",
    "    logging.info('Read config file.')\n",
    "    with open(args.configfile, \"r\") as cf:\n",
    "        config = yaml.load(cf, Loader=yaml.FullLoader)\n",
    "    sparksession = config['sparksession']\n",
    "    data = config['data']\n",
    "    output = config['output']\n",
    "\n",
    "    # Initiate the Spark session\n",
    "    logging.info('Instantiate the {0} Spark session'.format(sparksession['appName']))\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(sparksession['master']) \\\n",
    "        .appName(sparksession['appName']) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Load Data\n",
    "    logging.info('Load data to score')\n",
    "    datatoscore = read_parquet(spark, data['datatoscore_path'])\n",
    "\n",
    "    # Load Model\n",
    "    logging.info('Load trained pipeline')\n",
    "    pipemodel = load_model(output['pipeline_path'])\n",
    "\n",
    "    # Score data\n",
    "    datascored = score_model(datatoscore, pipemodel)\n",
    "    datascored.select(output['showschema_score']).show(5)\n",
    "\n",
    "    # Store scored data\n",
    "    logging.info('Save all the process outputs')\n",
    "    datascored_fmt = datascored.withColumn('P_Unusual0', extract0_udf('probability')).withColumn(\n",
    "         'P_Unusual1', extract1_udf('probability')).select(output['columnschema_score'])\n",
    "    write_parquet(datascored_fmt, output['datascored_path'])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(format='%(asctime)s %(name)-12s %(levelname)-8s %(message)s',\n",
    "                        datefmt='%m/%d/%Y %I:%M:%S %p', level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    parser = argparse.ArgumentParser(description=\"Score with Pyspark GBTClassifier\")\n",
    "    parser.add_argument('--configfile', required=True, help='path to configuration yaml file')\n",
    "    args = parser.parse_args()\n",
    "    run_scoring(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others\n",
    "\n",
    "Here we have to contact IT to submit gs path correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../deliverables/pyspark_GBTClassifier/demo-config.yml\n",
    "sparksession:\n",
    "  master: 'local[*]'\n",
    "  appName: 'Anomaly Detection'\n",
    "data:\n",
    "  train_datapath: gs://network-spark-migrate/data/ML-MATT-CompetitionQT1920_train_processed.parquet\n",
    "  test_datapath: gs://network-spark-migrate/data/ML-MATT-CompetitionQT1920_test_processed.parquet\n",
    "  datatoscore_path: gs://network-spark-migrate/data/ML-MATT-CompetitionQT1920_val_processed.parquet\n",
    "pipeline:\n",
    "  variables:\n",
    "    categoricals: ['hour', 'minutes']\n",
    "    numericals: ['PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL', 'maxThr_DL', 'maxThr_UL', 'meanUE_DL', 'meanUE_UL', 'maxUE_DL', 'maxUE_UL']\n",
    "  metadata:\n",
    "    index_names: ['hour_index', 'minutes_index']\n",
    "    encoded_names: ['hour_encoded', 'minutes_encoded']\n",
    "    vect_name: 'vars_vectorized'\n",
    "    feats_name: 'features'\n",
    "  model:\n",
    "    method: 'GBTClassifier'\n",
    "    labelCol: 'Unusual'\n",
    "    maxDepth: 5\n",
    "    maxBins: 32\n",
    "    maxIter: 3\n",
    "    seed: 888\n",
    "output:\n",
    "  showschema_train: ['CellName', 'features', 'Unusual', 'rawPrediction', 'probability', 'prediction']\n",
    "  showschema_score: ['CellName', 'features', 'rawPrediction', 'probability', 'prediction']\n",
    "  columnschema_train: ['CellName', 'Unusual', 'hour', 'minutes', 'PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL', 'maxThr_DL', 'maxThr_UL', 'meanUE_DL', 'meanUE_UL', 'maxUE_DL', 'maxUE_UL', 'P_Unusual0', 'P_Unusual1']\n",
    "  columnschema_score: ['CellName', 'hour', 'minutes', 'PRBUsageUL', 'PRBUsageDL', 'meanThr_DL', 'meanThr_UL', 'maxThr_DL', 'maxThr_UL', 'meanUE_DL', 'meanUE_UL', 'maxUE_DL', 'maxUE_UL', 'P_Unusual0', 'P_Unusual1']\n",
    "  test_scored_path: gs://network-spark-migrate/output/data/\n",
    "  metrics_scored_path: gs://network-spark-migrate/output/metrics/\n",
    "  pipeline_path: gs://network-spark-migrate/output/model/\n",
    "  datascored_path: gs://network-spark-migrate/output/scored/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to package and ship all to SAS Model Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip gbt_pipeline folder\n",
    "zip_folder(gbt_pipe_dir, rmtree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip deliverables folder\n",
    "model_zipath = zip_folder(model_version_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectname='Network anomaly detection'\n",
    "modelname='pyspark_GBTClassifier'\n",
    "np = False\n",
    "\n",
    "with Session(hostname=host, username=username, password=password, verify_ssl=False):\n",
    "    \n",
    "    if np == True:\n",
    "        model_repository.create_project(project=projectname,\n",
    "                                    repository='Public',\n",
    "                                    function='classification'\n",
    "                                    )\n",
    "\n",
    "    zipfile = open(model_zipath, 'rb')\n",
    "\n",
    "    model_repository.import_model_from_zip(modelname,\n",
    "                                           projectname,\n",
    "                                           file=zipfile,\n",
    "                                           version='new'\n",
    "                                           )\n",
    "    zipfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We version the model in SAS Model Manager. Now we are almost ready to migrate the workload on Google Cloud Platform."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
